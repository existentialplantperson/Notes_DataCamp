{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b218476",
   "metadata": {},
   "source": [
    "### Data Camp - Preprocessing Data for Machine Learning\n",
    "\n",
    "Data preprocessing\n",
    "- Beyond cleaning and exploratory data analysis\n",
    "- prepping data for modeling\n",
    "- python ML modeling requires numerical input\n",
    "\n",
    "- Understand data with:\n",
    "        df.head()\n",
    "        df.columns()\n",
    "        df.dtypes()\n",
    "        df.describe()\n",
    "\n",
    "- First step in preprocessing-remove missing data\n",
    "        df.dropna() to drop all rows with a missing value\n",
    "        df.drop([1,2,3]) to drop specific rows by index label\n",
    "        df.drop('col label', axis=1) to drop columns\n",
    "        df.dropna(axis=1, thresh=# of null to allow) axis 0 = row, axis 1 = col\n",
    "\n",
    "- Filter dataframe based on values\n",
    "        df[df['col'] == x]\n",
    "\n",
    "- Got a count of null values in a column then create a df that has those rows removed where the specified col has a null value\n",
    "        df['col'].isnull().sum()\n",
    "        df[df['col].notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb74533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop features/columns that have at least 3 missing values.\n",
    "volunteer.dropna(axis=1, thresh=3)\n",
    "\n",
    "# Check how many values are missing in the category_desc column\n",
    "print(volunteer['category_desc'].isnull().sum())\n",
    "\n",
    "# Subset the volunteer dataset by indexing by where category_desc is notnull()\n",
    "volunteer_subset = volunteer[volunteer['category_desc'].notnull()]\n",
    "\n",
    "# Print out the shape of the subset\n",
    "print(volunteer_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14be65ff",
   "metadata": {},
   "source": [
    "#### Working with Data Types\n",
    "\n",
    "df.dtypes\n",
    "- most commonly used in pandas are\n",
    "- object - string/mixed\n",
    "- int64 - integer\n",
    "- float64 - float\n",
    "- datetime64 (timedelta) - datetime\n",
    "\n",
    "- converting column types\n",
    "        df['col'] = df['col'].astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head of the hits column\n",
    "print(volunteer[\"hits\"].head())\n",
    "\n",
    "# Convert the hits column to type int\n",
    "volunteer[\"hits\"] = volunteer['hits'].astype('int')\n",
    "\n",
    "# Look at the dtypes of the dataset\n",
    "print(volunteer.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce7ba8",
   "metadata": {},
   "source": [
    "#### Training and Test Sets\n",
    "\n",
    "- function splits 75% into training and 25% into testing sets\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x,y)\n",
    "\n",
    "if you have imbalanced classes, stratified sampling, takes into account the distribution of classes in dataset\n",
    "ex: if data set contains 100 samples, 80 class 1, 20 class 2\n",
    "    - we want training set to contain 75 samples, 60 class 1 / 15 class 2\n",
    "    - test set containing 25 samples, 20 class 1, 5 class 2\n",
    "    \n",
    "can use the stratify parameter in train_test_split:\n",
    " - stratify = y\n",
    " - check value_counts\n",
    " \n",
    "Code example below - We know that the distribution of variables in the category_desc column in the volunteer dataset is uneven. If we wanted to train a model to try to predict category_desc, we would want to train the model on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdfc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data with all columns except category_desc\n",
    "volunteer_X = volunteer.drop(\"category_desc\", axis=1)\n",
    "\n",
    "# Create a category_desc labels dataset\n",
    "volunteer_y = volunteer[[\"category_desc\"]]\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)\n",
    "\n",
    "# Print out the category_desc counts on the training y labels\n",
    "print(y_train[\"category_desc\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd433eb4",
   "metadata": {},
   "source": [
    "## Chapter 2 \n",
    "#### Standardizing Data\n",
    "standardization: preprocessing method that transforms continuous numerical data to make it normally distributed\n",
    "- can be a prerequesite for many models (including scikit0learn models), skipping this step can bias the model\n",
    "- log normalization and feature scaling\n",
    "\n",
    "model in linear space, data must be in linear space\n",
    "- like KNN, linear regression, K-means clustering\n",
    "\n",
    "other models for nonlinear space\n",
    "\n",
    "standardization is needed when:\n",
    "- dataset features have high variance\n",
    "- dataset features are continuous and on different scales\n",
    "- linearity assumptions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3656e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeling without normalizing\n",
    "#column = Proline, has an extremely high variance compared to the other columns.\n",
    "#knn=KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))\n",
    "\n",
    "#low accuracy becasue we did not standardize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1966603e",
   "metadata": {},
   "source": [
    "#### Log Normalization\n",
    "method for standardizing when you have a column with high variance\n",
    "- applies log transformation (approximates normality)\n",
    "- if value = 30, log transformation = 3.4 because  e(2.718)^3.4 = 30\n",
    "- captures relative changes, magnitude of change, maintains positive values\n",
    "\n",
    "- Can check variance of data before transforming\n",
    "        df.var()\n",
    "        df['log_2'] = np.log(df['col2'])\n",
    "- check variance again and col 1 and col2 should be closer to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proline has a large amount of variance, let's log normalize it\n",
    "# Print out the variance of the Proline column\n",
    "print(wine['Proline'].var())\n",
    "#99166.71735542436\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine['Proline_log'] = np.log(wine['Proline'])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine['Proline_log'].var())\n",
    "#0.17231366191842012"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57329932",
   "metadata": {},
   "source": [
    "#### Scaling Data\n",
    "- useful when features are on different scale and using a linear model\n",
    "- centers features around 0 and transforms unit variance (approximates normal dist.)\n",
    "- required for many models in scikit.learn\n",
    "\n",
    "- StandardScaler object can apply the same transformation on new sets without rescaling everything\n",
    "        #create object\n",
    "        scaler = StandardScaler()\n",
    "        #convert array to df \n",
    "        df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "        #print the variance for each column, should be equal\n",
    "        df_scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef675671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash', \"Alcalinity of ash\", 'Magnesium']]\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "wine_subset_scaled = ss.fit_transform(wine_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc9ed6",
   "metadata": {},
   "source": [
    "#### Standardized data and modeling\n",
    "KNN - model classifies data based on class the majority of surrounding data points belong to\n",
    "\n",
    "do preprocess and train test split first\n",
    "\n",
    "knn.score(X_test, y_test) to evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try KNN WITHOUT standardizing data\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))\n",
    "\n",
    "#OUTPUT = 0.6444444444444445\n",
    "#low score without standardiaztion \n",
    "\n",
    "\n",
    "#KNN on scaled data\n",
    "# Create the scaling method.\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Apply the scaling method to the dataset used for modeling.\n",
    "X_scaled = ss.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data.\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data.\n",
    "print(knn.score(X_test, y_test))\n",
    "\n",
    "#OUTPUT = 0.9555555555555556 \n",
    "#much higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce341d",
   "metadata": {},
   "source": [
    "### Chapter 3\n",
    "#### Feature engineering\n",
    "\n",
    "creation of new features based on existing one\n",
    "- useful for prediction/clustering\n",
    "- insight into relationships between features\n",
    "\n",
    "in addition to preprocessing, you will likely need to extract and expand data\n",
    "will cover manual methods (automater wasy exist as well)\n",
    "\n",
    "dataset dependent, must have good knowledge of dataset\n",
    "examples:\n",
    "- text data for natural language processing\n",
    "- string data, encode into numerical data\n",
    "- timestamps, reducing from seconds to day or month\n",
    "\n",
    "#### Encoding categorical variables\n",
    "\n",
    "- Encoding binary variables - Pandas .apply()\n",
    "        df['new_col'] = df['col'].apply(lambda val: 1 if val == 'yes' else 0)\n",
    "\n",
    "- Encoding binary variables - scikit-learn\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        df['new_col'] = le.fit_transform(df['col'])\n",
    "\n",
    "Multiple categories to encode - One-hot encoding\n",
    "- transforms each unique value into an array of 0 and 1\n",
    "        pd.get_dummies(df['col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48659073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding categorical variables - binary\n",
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[['Accessible', 'Accessible_enc']].head())\n",
    "\n",
    "\n",
    "#Encoding categorical variables - one-hot\n",
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer['category_desc'])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "print(category_enc.head())\n",
    "\n",
    "OUTPUT:\n",
    "   Education  Emergency Preparedness  Environment  Health  Helping Neighbors in Need  Strengthening Communities\n",
    "0          0                       0            0       0                          0                          0\n",
    "1          0                       0            0       0                          0                          1\n",
    "2          0                       0            0       0                          0                          1\n",
    "3          0                       0            0       0                          0                          1\n",
    "4          0                       0            1       0                          0                          0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca39c948",
   "metadata": {},
   "source": [
    "#### Engineering numerical features\n",
    "can take an aggregate of a set of numbers to replace many values that are close together in time\n",
    "\n",
    "- EX: daily rainfall to get weekly average\n",
    "        df['mean'] = df.apply(lambda row: row['columns].mean(), axis=1\n",
    "\n",
    "Dates\n",
    "- be sure the column is converted to datetime\n",
    "        df['date_converted'] = pd.to_datetime(df['date'])\n",
    "        extract month from datetime format\n",
    "        df['month'] = df['date_converted].apply(lambda row: row.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engineering numerical features - taking an average\n",
    "#For each name in the dataset, take the mean of their 5 run times.\n",
    "\n",
    "print(running_times_5k)\n",
    "OUTPUT\n",
    "      name  run1  run2  run3  run4  run5\n",
    "0      Sue  20.1  18.5  19.6  20.3  18.3\n",
    "1     Mark  16.5  17.1  16.9  17.6  17.3\n",
    "2     Sean  23.5  25.1  25.2  24.6  23.9\n",
    "3     Erin  21.7  21.1  20.9  22.1  22.2\n",
    "4    Jenny  25.8  27.1  26.1  26.7  26.9\n",
    "5  Russell  30.9  29.6  31.4  30.4  29.9\n",
    "\n",
    "# Create a list of the columns to average\n",
    "run_columns = ['run1', 'run2', 'run3', 'run4', 'run5']\n",
    "\n",
    "# Use apply to create a mean column\n",
    "running_times_5k[\"mean\"] = running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)\n",
    "\n",
    "# Take a look at the results\n",
    "print(running_times_5k['mean'])\n",
    "\n",
    "OUTPUT\n",
    "      name  run1  run2  run3  run4  run5   mean\n",
    "0      Sue  20.1  18.5  19.6  20.3  18.3  19.36\n",
    "1     Mark  16.5  17.1  16.9  17.6  17.3  17.08\n",
    "2     Sean  23.5  25.1  25.2  24.6  23.9  24.46\n",
    "3     Erin  21.7  21.1  20.9  22.1  22.2  21.60\n",
    "4    Jenny  25.8  27.1  26.1  26.7  26.9  26.52\n",
    "5  Russell  30.9  29.6  31.4  30.4  29.9  30.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788fd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engineering numerical features - datetime\n",
    "#look at the start_date_date column and extract just the month\n",
    "\n",
    "# First, convert string column to date column\n",
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer['start_date_date'])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "volunteer[\"start_date_month\"] = volunteer['start_date_converted'].apply(lambda row: row.month)\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "print(volunteer[['start_date_converted', 'start_date_month']].head())\n",
    "\n",
    "OUTPUT\n",
    "  start_date_converted  start_date_month\n",
    "0           2011-07-30                 7\n",
    "1           2011-02-01                 2\n",
    "2           2011-01-29                 1\n",
    "3           2011-02-14                 2\n",
    "4           2011-02-05                 2"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5211814d",
   "metadata": {},
   "source": [
    "#### Engineering features from text - text classification\n",
    "\n",
    "extract pieces you need, a string or number\n",
    "\n",
    "transform text itself into features - language processing, prediction\n",
    "\n",
    "- extraction from text with .compile()\n",
    "        import re\n",
    "        my_string = \"temperature:75.6 F\"\n",
    "        pattern = re.compile(\"\\d+\\.\\d+\")\n",
    "        \\d+  = collect all digits before decimal\n",
    "        \\. collect decimal\n",
    "        \\d+ collect all digits after decimal\n",
    "        temp = re.match(pattern, my_string) - search string for matching pattern\n",
    "        print(float(temp.group(0)) - extract with group\n",
    "    \n",
    "vectorizing text - encode numerically with tfidf vector\n",
    "- ex: using document text for vectorization\n",
    "- reflects the importance of a term beside by frequency\n",
    "- tf = term frequency\n",
    "- idf = inverse document frequency\n",
    "\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        tfidf_vec = TfidfVectorizer()\n",
    "        text_tfidf = tfidf_vec.fit_transform(df)\n",
    "\n",
    "Text classification - Naive Bayes Classifier\n",
    "- treats each feature as independent of others, works well for text and high dimensional data\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ff24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engineering features from strings - extraction\n",
    "#The Length column in the hiking dataset is a column of strings, \n",
    "#but contained in the column is the mileage for the hike\n",
    "print(hiking['Length'].head())\n",
    "OUTPUT \n",
    "0     0.8 miles\n",
    "1      1.0 mile\n",
    "2    0.75 miles\n",
    "3     0.5 miles\n",
    "4     0.5 miles\n",
    "\n",
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    pattern = re.compile(r\"\\d+\\.\\d+\")\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.match(pattern, length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking[\"Length\"].apply(lambda row: return_mileage(row))\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())\n",
    "\n",
    "OUTPUT\n",
    "           Length  Length_num\n",
    "    0   0.8 miles        0.80\n",
    "    1    1.0 mile        1.00\n",
    "    2  0.75 miles        0.75\n",
    "    3   0.5 miles        0.50\n",
    "    4   0.5 miles        0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8faf1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engineering features from strings - tf/idf\n",
    "# Take the title text\n",
    "title_text = volunteer['title']\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "#Use the tfidf_vec vectorizer's fit_transform() function on title_text \n",
    "# to transform the text into a tf-idf vector.\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
    "\n",
    "\n",
    "#Text classification using tf/idf vectors\n",
    "#Using train_test_split, split the text_tfidf vector, \n",
    "#along with your y variable, into training and test sets. \n",
    "#Set the stratify parameter equal to y, since the class distribution is uneven. \n",
    "#Notice that we have to run the toarray() method on the tf/idf vector, \n",
    "#in order to get in it the proper format for scikit-learn.\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y)\n",
    "\n",
    "#Use Naive Bayes' fit() method on the X_train and y_train variables.\n",
    "# Fit the model to the training data\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(X_test, y_test))\n",
    "\n",
    "OUTPUT = 0.567741935483871\n",
    "#more feature selection later to increase score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6f4bf",
   "metadata": {},
   "source": [
    "## Chapter 4\n",
    "#### Feature selection\n",
    "\n",
    "select features from existing feature set to improve model performance\n",
    "- automated feature selection exists, covering manual selection\n",
    "- Examples:\n",
    "    - redundant features adding noise, i.e city/state & lat/long\n",
    "    - some features may be strongly correlated, breaking independent variable assumption of some models\n",
    "    - text vectors, use tfidf \n",
    "    - large feature set, use dimensionality reduction to reduce overall variance\n",
    " \n",
    "Removing redundant features\n",
    "- remove noisy features\n",
    "- remove highly correlated features\n",
    "- remove duplicate features\n",
    "\n",
    "Correlated features\n",
    "- statistically correlated - feature move together directionally\n",
    "- linear models assume feature independence\n",
    "- pearson correlation coefficient - measure of this directionality, range is -1 to 1\n",
    "    - df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you explore the volunteer dataset in the console, you'll see three features \n",
    "#which are related to location: locality, region, and postalcode\n",
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"category_desc\", \"created_date\", \"locality\", \"region\", \"vol_requests\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of the new dataset\n",
    "print(volunteer_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ab41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for correlated features\n",
    "# Print out the column correlations of the wine dataset\n",
    "print(wine.corr())\n",
    "\n",
    "# Take a minute to find the column where the correlation value is greater than 0.75 at least twice\n",
    "to_drop = \"Flavanoids\"\n",
    "\n",
    "# Drop that column from the DataFrame\n",
    "wine = wine.drop('Flavanoids', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec6fb1",
   "metadata": {},
   "source": [
    "#### Selecting features using text vectors\n",
    "don't necessarily need all words to train a model, could take the top 20% of weighted words to train\n",
    "\n",
    "test subset of tfidf to find out what works\n",
    "- how do we pull out words and weights to compare\n",
    "\n",
    "after vectorizing text, vocabulary and weights are stored in vectorizer\n",
    "- pull vocab list to look at word weights, underscore intentional\n",
    "        print(tfidf_vec.vocabulary_)\n",
    "- access items in the vocabulary like a list with 0 indexing\n",
    "        print(text_tfdif[3].data)\n",
    "- get indices of words that have been weighted\n",
    "        print(text_tfidf[3].indices)\n",
    "- reverse the key, value pairs to match conventional python formatting\n",
    "        vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
    "- zip together row indeces and weights and turn into dictionary\n",
    "        zipped_rows = dict(zip(text_tfidf[3].indices, text_tfidf[3].data))\n",
    "- can do this all in a function\n",
    "        def return_weights(vocab, vector, vector_index):\n",
    "        zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data\n",
    "        return {vocab[i]:zipped[i] for i in vector[vector_index].indices}\n",
    "- call function\n",
    "        print(return_weights(vocab, text_tfidf, 3))\n",
    "- sort by score or eliminate words below a certain threshhold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b08045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the rest of the parameters\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Let's transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Let's sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))\n",
    "\n",
    "\n",
    "#Using the function we wrote in the previous exercise, \n",
    "#we're going to extract the top words from each document in the text vector, \n",
    "#return a list of the word indices, \n",
    "#and use that list to filter the text vector down to those top words.\n",
    "\n",
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1926957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Naive Bayes with feature selection\n",
    "#Use train_test_split on the filtered_text text vector, the y labels (which is the category_desc labels), \n",
    "#and pass the y set to the stratify parameter, since we have an uneven class distribution.\n",
    "# Split the dataset according to the class distribution of category_desc, using the filtered_text vector\n",
    "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(train_X,train_y)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(test_X,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a627a8",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction\n",
    " - another way to reduce size of feature set\n",
    " - unsupervied learning method\n",
    " - combines/dcecomposes a feature space\n",
    " - feature extraction - used here to reduce feature space\n",
    " - PCA - principal component analysis\n",
    " - linear transformation to uncorrelated space\n",
    " - captures as much variance as possible in each component\n",
    " - useful for large number of features and no easy way to eliminate\n",
    " \n",
    " PCA\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA()\n",
    "        df_pca = pca.fit_transform(df)\n",
    "        print(df_pca)\n",
    "        - percentage of variance explained by that component\n",
    "        print(pca.explained_variance_ratio_)\n",
    "- can be difficult to interpret components beyond the ones with highest explained variance\n",
    "- black box method\n",
    "- end of preprocessing because of the way it is reshaped\n",
    "- mostlyu good for eliminating components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558beb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set up PCA and the X vector for diminsionality reduction\n",
    "pca = PCA()\n",
    "wine_X = wine.drop(\"Type\", axis=1)\n",
    "\n",
    "# Apply PCA to the wine dataset X vector\n",
    "transformed_X = pca.fit_transform(wine_X)\n",
    "\n",
    "# Look at the percentage of variance explained by the different components\n",
    "print(pca.explained_variance_ratio_)\n",
    "OUTPUT:\n",
    " [9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05\n",
    " 1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06\n",
    " 1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07\n",
    " 8.25392788e-08]\n",
    "\n",
    "\n",
    "#Training a model with PCA\n",
    "# Split the transformed X and the y labels into training and test sets\n",
    "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X, y)\n",
    "\n",
    "# Fit knn to the training data\n",
    "knn.fit(X_wine_train, y_wine_train)\n",
    "\n",
    "# Score knn on the test data and print it out\n",
    "knn.score(X_wine_test, y_wine_test)\n",
    "\n",
    "OUTPUT: 0.7555555555555555"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7488a39c",
   "metadata": {},
   "source": [
    "### Chapter 5\n",
    "#### Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae917ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column types, look at the data\n",
    "print(ufo.dtypes)\n",
    "\n",
    "# Change the type of seconds to float\n",
    "ufo[\"seconds\"] = ufo[\"seconds\"].astype(float)\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])\n",
    "\n",
    "# Check the column types\n",
    "print(ufo[[\"seconds\", \"date\"]].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many values are missing in the length_of_time, state, and type columns\n",
    "print(ufo[[\"length_of_time\", \"state\", \"type\"]].isnull().sum())\n",
    "\n",
    "# Keep only rows where length_of_time, state, and type are not null\n",
    "ufo_no_missing = ufo[ufo[\"length_of_time\"].notnull() & \n",
    "          ufo[\"state\"].notnull() & \n",
    "          ufo[\"type\"].notnull()]\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "print(ufo_no_missing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943234e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54c3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6cd3242",
   "metadata": {},
   "source": [
    "#### Categorical variables and Standardization\n",
    "- one hot encode with:\n",
    "        pd.get_dummies()\n",
    "- check variance then standardize with:\n",
    "        var()  \n",
    "        np.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting numbers from strings\n",
    "def return_minutes(time_string):\n",
    "    \n",
    "    #use \\d+ to grab digits and match it to the column values\n",
    "    pattern = re.compile(r\"\\d+\")\n",
    "        \n",
    "    # Use match on the pattern and column\n",
    "    num = re.match(pattern, time_string)\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "        \n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "print(ufo[[\"length_of_time\", \"minutes\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fbac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying features for standardization\n",
    "#investigate the variance of columns in the UFO dataset \n",
    "#to determine which features should be standardized\n",
    "# Check the variance of the seconds and minutes columns\n",
    "print(ufo[['seconds', 'minutes']].var())\n",
    "\n",
    "# Log normalize the seconds column\n",
    "ufo[\"seconds_log\"] = np.log(ufo['seconds'])\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo['seconds_log'].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566dd7ac",
   "metadata": {},
   "source": [
    "#### Engineering new features\n",
    "- extract month from dat\n",
    "        .month or .hour\n",
    "- extract minutes value from string\n",
    "        regex(regular expressions \\d)\n",
    "        .group() to return results\n",
    "- vectorize text in descripton\n",
    "        tf-idf and IfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding categorical variables\n",
    "# Use Pandas to encode us values as 1 and others as 0\n",
    "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda x: 1 if x=='us' else 0)\n",
    "\n",
    "# Print the number of unique type values\n",
    "print(len(ufo['type'].unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo['type'])\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting features from dates\n",
    "# Look at the first 5 rows of the date column\n",
    "print(ufo['date'].head())\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo[\"month\"] = ufo[\"date\"].apply(lambda row: row.month)\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo[\"year\"] = ufo[\"date\"].apply(lambda row: row.year)\n",
    "\n",
    "# Take a look at the head of all three columns\n",
    "print(ufo[['date', 'month', 'year']].head())\n",
    "\n",
    "OUTPUT \n",
    "                 date  month  year\n",
    "0 2002-11-21 05:45:00     11  2002\n",
    "1 2012-06-16 23:00:00      6  2012\n",
    "2 2013-06-09 00:00:00      6  2013\n",
    "3 2013-04-26 23:27:00      4  2013\n",
    "4 2013-09-13 20:30:00      9  2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text vectorization\n",
    "# Take a look at the head of the desc field\n",
    "print(ufo['desc'].head())\n",
    "\n",
    "# Create the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# Use vec's fit_transform method on the desc field\n",
    "desc_tfidf = vec.fit_transform(ufo['desc'])\n",
    "\n",
    "# Look at the number of columns this creates\n",
    "print(desc_tfidf.shape)\n",
    "\n",
    "OUTPUT: (1866, 3422)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81efc7",
   "metadata": {},
   "source": [
    "#### Feature selection and modeling\n",
    "- eliminate redundant features\n",
    "        in original form\n",
    "        due to feature engineering\n",
    "- inspect text vector and eliminate words\n",
    "- preprocessing are iterative practices, play around to discover the best model fro your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the ideal dataset\n",
    "\n",
    "# Check the correlation between the seconds, seconds_log, and minutes columns\n",
    "print(ufo[['seconds', 'seconds_log', 'minutes']].corr())\n",
    "\n",
    "# Make a list of features to drop\n",
    "to_drop = ['city', 'country', 'date', 'desc', 'lat', 'length_of_time', 'long', 'minutes', 'recorded', 'seconds', 'state']\n",
    "\n",
    "# Drop those features\n",
    "ufo_dropped = ufo.drop(to_drop, axis=1)\n",
    "\n",
    "# Let's also filter some words out of the text vector we created\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aacdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeling the UFO dataset\n",
    "#build a k-nearest neighbor model to predict which country \n",
    "#the UFO sighting took place in.\n",
    "\n",
    "# Take a look at the features in the X set of data\n",
    "print(X.columns)\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "train_X, test_X, train_y, test_y = train_test_split(X,y, stratify=y)\n",
    "\n",
    "# Fit knn to the training sets\n",
    "knn.fit(train_X, train_y)\n",
    "\n",
    "# Print the score of knn on the test sets\n",
    "print(knn.score(test_X, test_y))\n",
    "\n",
    "OUTPUT: 0.8758029978586723\n",
    "    \n",
    "#fair performance for predicting location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5034ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model using the text vector we created, desc_tfidf, \n",
    "#using the filtered_words list to create a filtered text vector. \n",
    "#predict the type of the sighting based on the text.\n",
    "#use a Naive Bayes model for this.\n",
    "\n",
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y \n",
    "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
    "\n",
    "# Fit nb to the training sets\n",
    "nb.fit(train_X, train_y)\n",
    "\n",
    "# Print the score of nb on the test sets\n",
    "nb.score(test_X, test_y)\n",
    "\n",
    "OUTPUT: 0.16059957173447537\n",
    "    #poor performance on text data\n",
    "    #next step would be to iterate through text data to improve performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
