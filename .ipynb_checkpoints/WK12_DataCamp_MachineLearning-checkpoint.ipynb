{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3b13be",
   "metadata": {},
   "source": [
    "## Machine Learning for Business - Data Camp\n",
    "\n",
    "- ML is applying statistical /computer science methods to data to\n",
    "1. draw causal insights\n",
    "    - what is causing customers to cancel subscription?\n",
    "2. predict future events\n",
    "    - which customers are likely to cancel their subscription\n",
    "3. understand patterns\n",
    "    - are there groups of customers who are similar\n",
    "    \n",
    "Data Heirarchy of Needs\n",
    "1. collection - extract data from source\n",
    "2. storage - reliable storage\n",
    "3. preparation - organize and clean data to make it usable\n",
    "4. analysis - understand trends, distributions, segments\n",
    "5. prototyping and testing ML - interpretable simple models, tests/experiments\n",
    "6. ML in production - complex models in production/research/automation\n",
    "\n",
    "Focus on 5 and 6 in this course\n",
    "\n",
    "#### Supervised vs. Unsupervised ML\n",
    "- Supervised - draw casual insights, predict future events\n",
    "- Unsupervised - understand patterns in data\n",
    "\n",
    "Supervised ML data structure:\n",
    "- Target variable - what we want to predict ex: fraud probability\n",
    "- Input features - list of columns with data ex: transaction data\n",
    "- SML uses the input features to predict the target variable\n",
    "\n",
    "Unsupervised ML data structure:\n",
    "- Uses input features (data points) to segment data\n",
    "\n",
    "Ex: Marketing\n",
    "- SML used to predict which cust. are likely to purchase next month, predict cust. lifetime value\n",
    "- USL used to group customers into segments based on past purchases\n",
    "\n",
    "\n",
    "#### Job roles, tools, and technologies\n",
    "\n",
    "Roles/Tools/Tech for Data Heirarchy of Needs\n",
    "1. collection - infrastructure owners\n",
    "2. storage - data engineers, database administrators\n",
    "3. preparation - data engineers / data analysts\n",
    "4. analysis - data analysts / data scientists\n",
    "5. prototyping and testing ML - data scientists / ML engineers\n",
    "6. ML in production - ML engineers\n",
    "\n",
    "Team Structure\n",
    "1. centralized - all data functions in one central team (small scale, maintains focus)\n",
    "2. decentralized - each business unit has own data function (can cause issues in overlap and silohs\n",
    "3. hybrid - infractructure, definitions, methods, tooling are centralized and application and prototyping is decentralized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af84e9ac",
   "metadata": {},
   "source": [
    "### Chapter 2\n",
    "#### Prediction vs. Inference Dilemma\n",
    "\n",
    "Inference or causal models \n",
    "- the goal is to understand the drivers of a business outcome\n",
    "- interpretable - easy to understand\n",
    "- less accurate\n",
    "- Ex: What are the main drivers of fraud? How much does condition X impact risk? What is the cause? what are the effects of conditions?\n",
    "\n",
    "Prediction\n",
    "- prediction itself is the main goal\n",
    "- not easily interpretable, black box\n",
    "- Ex: which transactions are likely fraudulent? Is the patient at risk of condition x? predictions based on variables?\n",
    "\n",
    "There is a trade-off between accuracy and interpretability: some models are easy to understand but do not have optimal predictions; others have excellent predictions but it's unclear how they got there.\n",
    "\n",
    "Modeling data structure\n",
    "- Ex: fraud probability, inference model is concerned with which input features affect fraud probability; prediction model is only concerned with accurately predicting fraud risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd24c49",
   "metadata": {},
   "source": [
    "#### Inference (causal) Models\n",
    "\n",
    "What is causality \n",
    "- identify causal relationship of how much a certain action affects an outcome of interest\n",
    "- answers the why question\n",
    "- optimizes for model interpretability vs. performance\n",
    "- models try to detect patterns in observed data and draw causal conclusions\n",
    "\n",
    "Experiments vs. Observations\n",
    "- experiments are designed and causal conclusions are guaranteed e.g. A/B tests\n",
    "- when experiments are impossible, the models (observational studies) are used to calculate effect of inputs on outcomes\n",
    "- experiments are preferred over observational studies\n",
    "\n",
    "Inference model example\n",
    "- data set of customers with spending observations (input features) and a target variable of next month spending\n",
    "- how much do the input variables affect the target\n",
    "- run a model to learn the prediction rules\n",
    "- regression coefficients for each varaiable tell us how much the input and target are related and wether +/-\n",
    "- ex: last month spending reg coef = 0.58 so the customers who spent on average 1USD more in the last month will spend 0.58 more in the next month compared with customers spending 1USD less last month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500e6d6",
   "metadata": {},
   "source": [
    "#### Prediction Models (Supervised Learning)\n",
    "- Classification: Predicting class/type of an outcome\n",
    "- Regression: predicting quantity of an outcome\n",
    "\n",
    "Supervised - Classification Model\n",
    "- target variable is categorical (discrete) (class of outcome)\n",
    "- Ex: will customer cancel subscription? Is transaction fraudulent? \n",
    "\n",
    "\n",
    "Supervised - Regression Model\n",
    "- target variable is continuous (continuous) (amount of outcome)\n",
    "- Number of product purchases? Dollars spent?\n",
    "\n",
    "#### Prediction Models (Unsupervised)\n",
    "- Clustering: grouping observatios\n",
    "- Anomaly detection - detecting which observations fall out of the discovered \"regular pattern\" and use it as an input in Supervised Learning or business input\n",
    "- recommender engines - eg Netflix movie recommendations\n",
    "\n",
    "Clustering example \n",
    "- segmentation\n",
    "- training\n",
    "- discover clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a3088",
   "metadata": {},
   "source": [
    "### Chapter 3\n",
    "#### Business requirements\n",
    "\n",
    "1. What is the business situation (expansion, etc)\n",
    "2. What is the business opportunity and how big is it? (identify the right markets)\n",
    "3. What are the business actions we can take? (prioritize and investment)\n",
    "\n",
    "Ex: \n",
    "- situation: customer churn\n",
    "- opportunity: reduce churn rate by X%, resulting in Y USD revenue saved\n",
    "- action: ifentify and improve churn drivers (advertising, cust service, web site, errors), identify customers at risk and introduce retention campaigns\n",
    "\n",
    "Always start with interence questions:\n",
    "- why has churn been increasing?\n",
    "- which information indicates a potential fraud?\n",
    "\n",
    "Build on inference with defined prediction questions:\n",
    "- can we identify customers at risk of churning?\n",
    "- can we flag potentially risky transactions?\n",
    "\n",
    "Business opportunity\n",
    "- size up the opportunity, Cost Benefit\n",
    "- once you know the drivers of the outcome, how much will it cost changing them and what will be the value of doing that?\n",
    "- run experiments with model predictions\n",
    "\n",
    "Actionable Machine Learning\n",
    "- look at historical levels\n",
    "- run experiments to see if you can affect the predicted outcome\n",
    "- if yes, calculate the opportiunity\n",
    "- if no, collect more data, do more research, narrow the question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ca3504",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "- underfitting - model is too simple and does not predict accurately\n",
    "- overfitting - model is too complex, memorizes data and will not predict unseen data well\n",
    "- good fit - does not oversimplify, some errors, but predicts unseen data well\n",
    "\n",
    "Assess model performance on test\n",
    "- model training on a subset of data, build a model which learns the rules; how to use inputs A, B... to predict target Y\n",
    "- measure model performance on UNSEEN test data\n",
    "- repeate until best test is found for a good fit\n",
    "\n",
    "#### Model Performance Measurement\n",
    "accuracy metrics - measures how well classification is predicted\n",
    "- accuracy = all correct predictions / all observations\n",
    "    - precision metrics - how far are the predictions from the real values in regression\n",
    "        - precision = correct predictions/predicted observations\n",
    "    - recall metrics\n",
    "        - recall = correct predictions/all actual observations\n",
    "\n",
    "Regression performance - predicting continuious variable\n",
    "- error metric is key metric measures how far raway the prediction is from observation\n",
    "\n",
    "\n",
    "Actionable models - A/B Testing\n",
    "- good models are not always actionable\n",
    "    -churn prediction, purchase prediction, machine failure prediction\n",
    "- test if using models helps improve outcomes\n",
    "    - target cust predicted to churn with incentives\n",
    "    - send remainders for likely to purchase cust\n",
    "- did the action result in a desired outcome - yes, build in, no - start over\n",
    "\n",
    "#### Machine Learning Risks\n",
    "poor performance - some models perform poorly (make sure to review test performance, not training)\n",
    "    - low precision - lots of false positives\n",
    "    - low recall - fraction of target is captured (recalled) by model\n",
    "    - large error - large differences between predicted and actual values\n",
    "\n",
    "Non-actionable model use cases\n",
    "- how to test the models correctly - run tests and experiments to validate performance\n",
    "- A/B testing - split a group identified by the market into two, experimental and control\n",
    "- run qualitative research (surveys, etc)\n",
    "- change scope of problem, narrow, widen, change questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c95e75",
   "metadata": {},
   "source": [
    "### Chapter 4\n",
    "#### Machine Learning Mistakes\n",
    "1. Machine Learning before proper prep\n",
    "    - remember the data heirarchy of needs\n",
    "    - garbage in, garbage out\n",
    "2. Target variable definition\n",
    "    - what are we predicting, can we observe it (i.e. contractual churn vs non-contractual churn), in depth analysis to shape the variable definition\n",
    "3. Feature selection\n",
    "    - inference (what affects the target variable?)\n",
    "        - choose variable that you can control\n",
    "        - business must be involved in feature selection\n",
    "    - prediction (can we estimate the target value in the future)\n",
    "        - start with readily available data\n",
    "        - new features after testing and keep testing\n",
    "4. Late testing, no impact\n",
    "    - test frequently to avoid getting too far on a model that cannot be validated\n",
    "    \n",
    "#### Communication management\n",
    "- working groups, schedule meetings to track progress\n",
    "    - define business requirements\n",
    "    - review ML model AND business products\n",
    "    - inference vs. orediction\n",
    "    - baseline model results and outline model updates\n",
    "    - market testing\n",
    "    - production\n",
    "- business requirements\n",
    "    - what is the business situation\n",
    "    - what is the business opportunity\n",
    "    - what business actions can we take\n",
    "- machine learning products\n",
    "    - what does the business need?\n",
    "    - ex: churn prediction model; business wants inference drivers of churn updated monthly and daily cust classification into  high, medium,  low risk of churn\n",
    "- model performance and improvements\n",
    "    - identify tolerance for model mistakes\n",
    "    - classification - which class is more expensive to mis-classify?\n",
    "    - regression - what is the error tolerance for prediction?\n",
    "- market testing\n",
    "    - identify early\n",
    "- machine learning in production\n",
    "    - are test results delivering improvements\n",
    "    - is model stable enough\n",
    "    - do we have systems and tools where model will be integrated\n",
    "    \n",
    "#### Machine Learning in Production\n",
    "- production system is live, customer facing, and business critical\n",
    "- ex: customer relationship management (crm) - predicted churn triggers automatic emails\n",
    "- fraud detection system - predicted fraud probability automatically triggers transaction block\n",
    "- online banking platform - multiple models to predict behavior liek recommender engine\n",
    "- autonomous cars - collision avoidance, braking and foreced steering\n",
    "\n",
    "Staffing\n",
    "- prototype ML (data scientists, ML engineers)\n",
    "- ML in production (software engineerrs, data engineers, infrastructure owners)\n",
    "\n",
    "Launch, tracking, feedback\n",
    "- launch on small subset of custoimers\n",
    "- track results\n",
    "- track performance, stabilty, feedback\n",
    "- scale up\n",
    "- repeat tracking, testing, scaling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
