{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf4ad0c",
   "metadata": {},
   "source": [
    "### WK08 Statistics in Python - DataCamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095990e",
   "metadata": {},
   "source": [
    "summary stastic - fact about or summary of some data\n",
    "\n",
    "Types of stastics\n",
    "- Descriptive - describe and summarize data\n",
    "- Inferential stastics - uses a sample to make inferences about a larger pop.\n",
    "\n",
    "Types of data:\n",
    "- Numeric (Quantitative)\n",
    "    - Continuous (Measured)\n",
    "    - Discrete (Counted)\n",
    "- Categorical (Qualitative)\n",
    "    - Nominal (Unordered)\n",
    "    - Ordinal (Ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40f593",
   "metadata": {},
   "source": [
    "### Measures of Center\n",
    "Mean - average, much more sensitive to extreme values\n",
    "- np.mean\n",
    "\n",
    "Median - center value, 50% above and 50% below, better to use if data is skewed\n",
    "- np.median\n",
    "\n",
    "Mode - most frequent value\n",
    "- stastics.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two DataFrames: one that holds the rows of food_consumption for 'Belgium' and another that holds rows for 'USA'. Call these be_consumption and usa_consumption.\n",
    "#Calculate the mean and median of kilograms of food consumed per person per year for both countries.\n",
    "\n",
    "# Filter for Belgium\n",
    "be_consumption = food_consumption[food_consumption['country'] == 'Belgium']\n",
    "\n",
    "# Filter for USA\n",
    "usa_consumption = food_consumption[food_consumption['country'] == 'USA']\n",
    "\n",
    "# Calculate mean and median consumption in Belgium\n",
    "print(np.mean(be_consumption['consumption']))\n",
    "print(np.median(be_consumption['consumption']))\n",
    "\n",
    "# Calculate mean and median consumption in USA\n",
    "print(np.mean(usa_consumption['consumption']))\n",
    "print(np.median(usa_consumption['consumption']))\n",
    "\n",
    "#Subset food_consumption for rows with data about Belgium and the USA.\n",
    "#Group the subsetted data by country and select only the consumption column.\n",
    "#Calculate the mean and median of the kilograms of food consumed per person per year in each country using .agg().\n",
    "\n",
    "# Subset for Belgium and USA only\n",
    "be_and_usa = food_consumption[(food_consumption['country'] == \"Belgium\") | (food_consumption['country'] == 'USA')]\n",
    "\n",
    "# Group by country, select consumption column, and compute mean and median\n",
    "print(be_and_usa.groupby('country')['consumption'].agg([np.mean, np.median]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN VS MEDIAN\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Subset for food_category equals rice\n",
    "rice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n",
    "\n",
    "# Histogram of co2_emission for rice and show plot\n",
    "rice_consumption['co2_emission'].hist()\n",
    "plt.show()\n",
    "\n",
    "# Calculate mean and median of co2_emission with .agg()\n",
    "print(rice_consumption['co2_emission'].agg([np.mean, np.median]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468f7461",
   "metadata": {},
   "source": [
    "### Measures of Spread\n",
    "Variance - average distance from data point to mean (np.var(df[col], ddof = 1)\n",
    "Standard deviation - square root of variance np.std(df[col], ddof=1)\n",
    "Mean absolute deviation - \n",
    "\n",
    "Quantiles/percentiles - percent data in some number of equal parts\n",
    "np.quantile(df[col], quantile#) or pass list of percentiles [0, 0.25, 0.5, 0.75, 1]  can use np.quantile(df[col], np.linspace(start, stop, num intervals) ex: (0, 1, 5)\n",
    "\n",
    "Find outliers\n",
    "outllier if data point < Q1 - 1.5 * IQR OR > Q3 + 1.5 * IQR\n",
    "\n",
    "Find outliers with \n",
    "from scipy.stats import iqr\n",
    "iqr = (df[col])\n",
    "\n",
    "lower_threshold = np.quantile(df[col], 0.25) - 1.5 * iqr\n",
    "\n",
    "upper_threshold = np.quantile(df[col], 0.75) + 1.5 * iqr\n",
    "\n",
    "df[(df[col] < lower_threshold) | (df[col] > upper_threshold)]\n",
    "\n",
    ".describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fab86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIANCE AND STANDARD DEVIATION\n",
    "\n",
    "# Calculate the quartiles of co2_emission\n",
    "print(np.quantile(food_consumption['co2_emission'], [0, 0.25, 0.5, 0.75, 1]))\n",
    "\n",
    "# Calculate the quintiles of co2_emission\n",
    "print(np.quantile(food_consumption['co2_emission'], [0, 0.2, 0.4, 0.6, 0.8, 1]))\n",
    "\n",
    "# Calculate the deciles of co2_emission\n",
    "print(np.quantile(food_consumption['co2_emission'], [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]))\n",
    "\n",
    "# Print variance and sd of co2_emission for each food_category\n",
    "print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create histogram of co2_emission for food_category 'beef'\n",
    "food_consumption['co2_emission'][food_consumption['food_category'] == 'beef'].hist()\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Create histogram of co2_emission for food_category 'eggs'\n",
    "food_consumption['co2_emission'][food_consumption['food_category'] == 'eggs'].hist()\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b230c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINDING OUTLIERS USING IQR\n",
    "\n",
    "# Calculate total co2_emission per country: emissions_by_country\n",
    "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n",
    "\n",
    "# Compute the first and third quartiles and IQR of emissions_by_country\n",
    "q1 = np.quantile(emissions_by_country, 0.25)\n",
    "q3 = np.quantile(emissions_by_country, 0.75)\n",
    "iqr = q3 - q1 \n",
    "\n",
    "# Calculate the lower and upper cutoffs for outliers\n",
    "lower = q1 - 1.5 * iqr\n",
    "upper = q3 + 1.5 * iqr\n",
    "\n",
    "# Subset emissions_by_country to find outliers\n",
    "outliers = emissions_by_country[(emissions_by_country < lower)|(emissions_by_country > upper)]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3075dcfd",
   "metadata": {},
   "source": [
    "### Part 2: Random Numbers and Probability\n",
    "P(event) = #ways an event can happen / total # of possible outcomes\n",
    "P(heads) = 1/2 = 50%\n",
    "probabililty range is 0-1 / never - always\n",
    "\n",
    "df.sample() -- chooses one row from the df\n",
    "df.random.seed(10)  --- will generate the same random number that was originally pulled\n",
    "\n",
    "sampling without replacement (default) OR \n",
    "sampling with replacement df.sample(# samples, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the deals for each product\n",
    "counts = amir_deals['product'].value_counts()\n",
    "print(counts)\n",
    "\n",
    "# Calculate probability of picking a deal with each product\n",
    "probs = counts / counts.sum()\n",
    "print(probs)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(24)\n",
    "\n",
    "# Sample 5 deals without replacement\n",
    "sample_without_replacement = amir_deals.sample(5)\n",
    "print(sample_without_replacement)\n",
    "\n",
    "# Sample 5 deals with replacement\n",
    "sample_with_replacement = amir_deals.sample(5, replace = True)\n",
    "print(sample_with_replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a898192",
   "metadata": {},
   "source": [
    "### Discrete distributions\n",
    "probability distribution describes the possibility of every outcome\n",
    "Expected value - mean of a distribution\n",
    "\n",
    "Probability P(die roll) <= 2 = 1/6 + 1/6 = 1/3\n",
    "\n",
    "Discrete outcomes - die are discrete - make discrete uniform distribution of probability\n",
    "\n",
    "sampling from discrete distributions - larger samples are closer to mean (law of large numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8467954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of restaurant_groups, setting bins to [2, 3, 4, 5, 6]\n",
    "restaurant_groups['group_size'].hist(bins = np.linspace(2, 6, 5))\n",
    "plt.show()\n",
    "\n",
    "# Create probability distribution\n",
    "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n",
    "\n",
    "# Reset index and rename columns\n",
    "size_dist = size_dist.reset_index()\n",
    "size_dist.columns = ['group_size', 'prob']\n",
    "\n",
    "# Expected value\n",
    "expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n",
    "\n",
    "# Subset groups of size 4 or more\n",
    "groups_4_or_more = size_dist[size_dist['group_size'] >= 4]\n",
    "\n",
    "# Sum the probabilities of groups_4_or_more\n",
    "prob_4_or_more = np.sum(groups_4_or_more['prob'])\n",
    "print(prob_4_or_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85992469",
   "metadata": {},
   "source": [
    "### Continuous Distribution\n",
    "\n",
    "Continuous Uniform distribution \n",
    "\n",
    "uniform.cdf(7, lower limit, upper limit) P(lower limit - 7)\n",
    "\n",
    "1 - uniform.cdf (7, lower, upper) = P(7 - upper limit)\n",
    "\n",
    "uniform.cdf(7, lower, upper) - uniform(4, lower, upper) = P(4-7)\n",
    "\n",
    "\n",
    "Generate random numbers acording to uniform distribution\n",
    "\n",
    "uniform.rvs(lower limit, upper limit, size = # numbers)\n",
    "\n",
    "Other typers of distribution:\n",
    "\n",
    "Normal dist., exponential dist. - area of dist is always 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b701897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min and max wait times for back-up that happens every 30 min\n",
    "min_time = 0\n",
    "max_time = 30\n",
    "\n",
    "# Import uniform from scipy.stats\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Calculate probability of waiting less than 5 mins\n",
    "prob_less_than_5 = uniform.cdf(5, 0, 30)\n",
    "print(prob_less_than_5)\n",
    "\n",
    "# Calculate probability of waiting more than 5 mins\n",
    "prob_greater_than_5 = 1- uniform.cdf(5, 0, 30)\n",
    "print(prob_greater_than_5)\n",
    "\n",
    "# Calculate probability of waiting 10-20 mins\n",
    "prob_between_10_and_20 = uniform.cdf(20, 0, 30) - uniform.cdf(10, 0, 30)\n",
    "print(prob_between_10_and_20)\n",
    "\n",
    "# Generate 1000 wait times between 0 and 30 mins\n",
    "wait_times = uniform.rvs(0, 30, 1000)\n",
    "\n",
    "# Create a histogram of simulated times and show plot\n",
    "plt.hist(wait_times)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89530ca8",
   "metadata": {},
   "source": [
    "### The Binomial distribution\n",
    "binary outcome - two possible values 1/0, win/loss\n",
    "\n",
    "use binom function\n",
    "binom.rvs(# of coins, probability of heads, size#) - returns number of heads\n",
    "n = number of trials\n",
    "p = probability of success\n",
    "\n",
    "distribution is a bell curve, probability of 5 heads in 10 flips is highest\n",
    "\n",
    "P(heads=7) binom.pmf( # heads, # trials, prob heads)\n",
    "\n",
    "P(heads<=7) binom.cdf(7, 10, 0.5)\n",
    "\n",
    "P(heads >7) 1 - binom.cdf(7, 10, 0.5)\n",
    "\n",
    "Expected value = n * p (# heads * # flips)\n",
    "\n",
    "Independence - each trial must be independent and have no affect on the next for binomial distribution to apply\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import binom from scipy.stats\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Set random seed to 10\n",
    "np.random.seed(10)\n",
    "\n",
    "# Simulate a single deal\n",
    "#To simulate 1 deal 1 time, use binom.rvs(), passing 1 as the first argument \n",
    "#and the probability of closing a deal as the second argument. \n",
    "#The size argument should be 1.\n",
    "print(binom.rvs(1, 0.3, size=1))\n",
    "\n",
    "# Simulate 1 week of 3 deals\n",
    "print(binom.rvs(3, 0.3, size=1))\n",
    "\n",
    "# Simulate 52 weeks of 3 deals\n",
    "deals = binom.rvs(3, 0.3, size=52)\n",
    "\n",
    "# Print mean deals won per week\n",
    "print(np.mean(deals))\n",
    "\n",
    "# Probability of closing 3 out of 3 deals\n",
    "prob_3 = binom.pmf(3, 3, 0.3)\n",
    "\n",
    "# Probability of closing <= 1 deal out of 3 deals\n",
    "prob_less_than_or_equal_1 = binom.cdf(1, 3, 0.3)\n",
    "\n",
    "# Probability of closing > 1 deal out of 3 deals\n",
    "prob_greater_than_1 = 1 - binom.cdf(1, 3, 0.3)\n",
    "\n",
    "# Expected number won of 3 with 30% win rate\n",
    "won_30pct = 0.3 * 3\n",
    "\n",
    "# Expected number won of 3 with 25% win rate\n",
    "won_25pct = 0.25 * 3\n",
    "\n",
    "# Expected number won of 3 with 35% win rate\n",
    "won_35pct = 0.35 * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747e9af8",
   "metadata": {},
   "source": [
    "### The normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c47c82",
   "metadata": {},
   "source": [
    "bell curve, symmetrical, area=1, curve never hits 0\n",
    "\n",
    "described by mean (highest point of curve) and std dev (IQR)\n",
    "\n",
    "standard normal distribution - mean =1, std dev = 1\n",
    "- 68% of areas w/in 1 std dev\n",
    "- 95% of area w/in 2 std devs\n",
    "- 99.7% or area within 3 std devs\n",
    "    \n",
    "from scipy.stats import norm\n",
    "- norm.cdf(# of interest, mean, std dev) = % below #of interest\n",
    "- 1 - norm.cdf(# of interest, mean, std dev) = % above #of interest\n",
    "- norm.cdf(high range, ...) - norm.cdf(low range...) = % within range\n",
    "\n",
    "calculate percentiles\n",
    "- norm.ppf(percentile, mean, std dev) = value under which 90% of sample falls\n",
    "- norm.ppf(1-percentile, mean, std dev) = value above which 90% of sample falls\n",
    "\n",
    "#generate random numbers in normal distribution\n",
    "- norm.rvs(mean, std dev, size = sample size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of amount with 10 bins and show plot\n",
    "amir_deals['amount'].hist(bins=10)\n",
    "plt.show()\n",
    "\n",
    "# Probability of deal < 7500, avg = 5000, std dev = 2000\n",
    "prob_less_7500 = norm.cdf(7500, 5000, 2000)\n",
    "\n",
    "# Probability of deal > 1000\n",
    "prob_over_1000 = 1 - norm.cdf(1000, 5000, 2000)\n",
    "\n",
    "# Probability of deal between 3000 and 7000\n",
    "prob_3000_to_7000 = norm.cdf(7000, 5000, 2000) - norm.cdf(3000, 5000, 2000)\n",
    "\n",
    "# Calculate amount that 25% of deals will be less than\n",
    "pct_25 = norm.ppf(0.25, 5000, 2000)\n",
    "\n",
    "# Calculate new average amount, increase 20%\n",
    "new_mean = 5000+(5000*.2)\n",
    "\n",
    "# Calculate new standard deviation, increase 30%\n",
    "new_sd = 2000+(2000*.3)\n",
    "\n",
    "# Simulate 36 new sales\n",
    "new_sales = norm.rvs(new_mean, new_sd, size=36)\n",
    "\n",
    "# Create histogram and show\n",
    "plt.hist(x=new_sales)\n",
    "plt.show()\n",
    "\n",
    "#based on the metric of percent of sales over $1000\n",
    "#Does Amir perform better of worse in the predicted new market\n",
    "#old market\n",
    "1 - norm.cdf(1000, 5000, 2000)\n",
    "\n",
    "#new market\n",
    "1 - norm.cdf(1000, 6000, 2600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d1234",
   "metadata": {},
   "source": [
    "### Part 3: The central limit thorem and more distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8741d",
   "metadata": {},
   "source": [
    "die = pd.Series([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "roll 5 times\n",
    "\n",
    "samp_5 = die.sample(5, replace = True)\n",
    "\n",
    "MEAN = np.mean(samp_5)\n",
    "\n",
    "roll 5 times and take mean each time , add to empty list\n",
    "\n",
    "sample_means[]\n",
    "\n",
    "for i in range (10)\n",
    "- samp_5 = die.sample(5, replace = True)\n",
    "- sample_means.append(np.mean(samp_5))\n",
    "\n",
    "Sampling distribution - as sample size increases, looks more like normal distribution\n",
    "\n",
    "Central limit theorem (CLT) -ths sampling distribution of a statistic becomes closer to the normal distribution as the number of trials increases. Only applies to independent and random samples.  Applies to std dev and proportions.\n",
    "\n",
    "mean of sampling distribution - used to estimate characteristics of unknown underlying distribution\n",
    " - more easily estimate characteristics of large populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b05343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of num_users and show\n",
    "amir_deals['num_users'].hist()\n",
    "plt.show()\n",
    "\n",
    "# Set seed to 104\n",
    "np.random.seed(104)\n",
    "\n",
    "# Sample 20 num_users with replacement from amir_deals\n",
    "samp_20 = amir_deals['num_users'].sample(20, replace = True)\n",
    "\n",
    "# Take mean of samp_20\n",
    "np.mean(samp_20)\n",
    "\n",
    "# Loop 100 times and add to empty list\n",
    "sample_means = []\n",
    "\n",
    "for i in range(100):\n",
    "  # Take sample of 20 num_users\n",
    "  samp_20 = amir_deals['num_users'].sample(20, replace=True)\n",
    "  # Calculate mean of samp_20\n",
    "  samp_20_mean = np.mean(samp_20)\n",
    "  # Append samp_20_mean to sample_means\n",
    "  sample_means.append(samp_20_mean)\n",
    "\n",
    "# Convert to Series and plot histogram\n",
    "sample_means_series = pd.Series(sample_means)\n",
    "sample_means_series.hist()\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "#compare mean of a sample of all deals to the mean pf amir's deals\n",
    "\n",
    "# Set seed to 321\n",
    "np.random.seed(321)\n",
    "\n",
    "sample_means = []\n",
    "# Loop 30 times to take 30 means\n",
    "for i in range(30):\n",
    "  # Take sample of size 20 from num_users col of all_deals with replacement\n",
    "  cur_sample = all_deals['num_users'].sample(20, replace=True)\n",
    "  # Take mean of cur_sample\n",
    "  cur_mean = np.mean(cur_sample)\n",
    "  # Append cur_mean to sample_means\n",
    "  sample_means.append(cur_mean)\n",
    "\n",
    "# Print mean of sample_means\n",
    "print(np.mean(sample_means))\n",
    "\n",
    "# Print mean of num_users in amir_deals\n",
    "print(np.mean(amir_deals['num_users']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cc2aa6",
   "metadata": {},
   "source": [
    "### The Poisson distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f5b26b",
   "metadata": {},
   "source": [
    "Events appear to happen at a certain rate, but sompletely at random \n",
    "\n",
    "A number of countable outcomes over a period of time\n",
    "- Ex: # ppl arriving at a restaurant per hour, # of earthquakes in CA\n",
    "- time unit is irrelevant, but should be consistant\n",
    "\n",
    "Poisson distribution - probability of some # of events occuring over a fixed period of time, described by Lambda(λ)\n",
    "- λ = average number of events per time interval\n",
    "- distribution peak is at λ value\n",
    "\n",
    "Probability of poisson dist.\n",
    "- from scipy.stats import poisson\n",
    "- poisson.pmf(# of interest, mean) = probability of # of interest\n",
    "- poisson.cdf(# of interest, mean) = probability of <= # of int\n",
    "- 1-poisson.cdf(# of interest, mean) = probability of > # of int\n",
    "\n",
    "Sampling from possion dist.\n",
    "- poisson.rvs(mean, size = sample size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf28c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import poisson from scipy.stats\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# Probability of 5 responses, avg 4\n",
    "prob_5 = poisson.pmf(5, 4)\n",
    "\n",
    "# Probability of 2 or fewer responses\n",
    "prob_2_or_less = poisson.cdf(2, 4)\n",
    "\n",
    "# Probability of > 10 responses\n",
    "prob_over_10 = 1 - poisson.cdf(10, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1260797b",
   "metadata": {},
   "source": [
    "### More Probability Distributions - Exponential, T, Log normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c6a6b",
   "metadata": {},
   "source": [
    "Exponential distribution \n",
    "- predict probability of time between poisson events\n",
    "- ex: probability of <10 minutes between restaurant arrivals, Probability of 6-8 months between earthquakes\n",
    "\n",
    "Ex; if 1 customer service ticket is created every 2 minuste\n",
    "- λ = 0.5 tickets per minute = rate (poisson)\n",
    "- 1/λ = 1 request per 2 minutes = time (exponential)\n",
    "\n",
    "probability\n",
    "- from scipy.stats import expon\n",
    "- expon.cdf(# int, scale= λ) = probability of < # of int\n",
    "- 1 - expon.cdf(#int, scale=λ) = probability of > # of int\n",
    "- expon.cdf(high range...) - expon.cdf(low range...) = prob. of #int btwn range\n",
    "\n",
    "\n",
    "Student's / T-distribution - similar appearance to normal distribution, but with thicker tails\n",
    "- degrees of freedom = df  \n",
    "- low df = thicker tails = higher std dev\n",
    "- higher df = closer to normal distribution\n",
    "\n",
    "\n",
    "Log normal distribution\n",
    "- variable whose logarithm is normally distributed\n",
    "- ex = length of chess games, adult blood pressure, # hospitalizations in outbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692124e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print probability response takes < 1 hour, avg = 2.5\n",
    "print(expon.cdf(1, scale=2.5))\n",
    "\n",
    "# Print probability response takes > 4 hours\n",
    "print(1 - expon.cdf(4, scale=2.5))\n",
    "\n",
    "# Print probability response takes 3-4 hours\n",
    "print(expon.cdf(4, scale = 2.5) - expon.cdf(3, scale = 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ace83",
   "metadata": {},
   "source": [
    "### Part 4: Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27516767",
   "metadata": {},
   "source": [
    "scatter plots show the relationship between two variables\n",
    "- x = explanatory / independent variable\n",
    "- y = response / dependent variable\n",
    "\n",
    "correlation coefficient = between -1 and 1, quantifies the linear relationship between two variables, magnituse corresponds to the strength of relationship\n",
    "- pos or neg = +/- relationship, increasing or decreasing\n",
    "- near 0 = no relationship\n",
    "\n",
    "- import seaborn as sns\n",
    "- sns.scatterplot(x = variable, y=variable, data=df)\n",
    "- plt.show()\n",
    "\n",
    "- add linear trend line, confidence interval margins = ci\n",
    "- sns.lmplot(x=variable, y=variable, data=df, ci=none)\n",
    "\n",
    "computing correlation\n",
    "- df[col1].corr(df.[col2]) - does not matter which is 1 or 2\n",
    "\n",
    "this method uses r = Pearson product-moment correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8386fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot of happiness_score vs. life_exp and show\n",
    "sns.scatterplot(x=\"life_exp\", y='happiness_score', data = world_happiness)\n",
    "plt.show()\n",
    "\n",
    "# Create scatterplot of happiness_score vs life_exp with trendline\n",
    "sns.lmplot(x='life_exp', y='happiness_score', data=world_happiness, ci=None)\n",
    "plt.show()\n",
    "\n",
    "#correlation is approx 0.8 (strong positive)\n",
    "# Correlation between life_exp and happiness_score\n",
    "cor = world_happiness['life_exp'].corr(world_happiness['happiness_score'])\n",
    "\n",
    "#correlation is approx 0.8 (strong positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396390b6",
   "metadata": {},
   "source": [
    "### Correlation caveats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95ffae",
   "metadata": {},
   "source": [
    "correlation coefficient only measures the relationship of linear data (not quadratic, exponential, etc)\n",
    "\n",
    "For highly skewed data, perform log transformation, log(x) then plot correlation\n",
    "- create new column: df[new_col] = np.log(df[col])\n",
    "\n",
    "Other transformations:\n",
    "- Square root transformation - sqrt(x)\n",
    "- Reciprocal transformation - 1/x)\n",
    "\n",
    "Combinations:\n",
    "- log(x) and log(y)\n",
    "- sqrt(x) and 1/y\n",
    "\n",
    "linear regression relys on linear relationships\n",
    "\n",
    "correlation does not imply causation\n",
    "\n",
    "confounder - lurking variable that is not beiong studied confuses a spurious relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f3a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of happiness_score vs. gdp_per_cap\n",
    "sns.scatterplot(x='gdp_per_cap', y='happiness_score', data=world_happiness)\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation\n",
    "cor = world_happiness['gdp_per_cap'].corr(world_happiness['happiness_score'])\n",
    "\n",
    "#not a linear relationship\n",
    "\n",
    "# Create log_gdp_per_cap column\n",
    "world_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])\n",
    "\n",
    "# Scatterplot of log_gdp_per_cap and happiness_score\n",
    "sns.scatterplot(x='log_gdp_per_cap', y='happiness_score', data=world_happiness)\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation\n",
    "cor = world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])\n",
    "print(cor)\n",
    "\n",
    "\n",
    "\n",
    "# Scatterplot of grams_sugar_per_day and happiness_score\n",
    "sns.scatterplot(x='grams_sugar_per_day', y='happiness_score', data=world_happiness)\n",
    "plt.show()\n",
    "\n",
    "# Correlation between grams_sugar_per_day and happiness_score\n",
    "cor = world_happiness['grams_sugar_per_day'].corr(world_happiness['happiness_score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94594315",
   "metadata": {},
   "source": [
    "### Design of experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b424dfc",
   "metadata": {},
   "source": [
    "experimental question - what is the effect of the treatment on the response\n",
    "- treatment- explanatory/independent variable\n",
    "- response- response/dependent variable\n",
    "\n",
    "controlled experiments - participants assigned to treatment or control group\n",
    "- experiments seek to eliminate as much bias as possible\n",
    "- randomized control trial\n",
    "- placebo\n",
    "- double blind experiment - person adminstering treatment doesn't know which group the participant is in\n",
    "\n",
    "observational studies - \n",
    "- participants self assign based on preesistying cond.\n",
    "- establish association, not causation\n",
    "- ways to control for confounder\n",
    "\n",
    "longitudinal studies vs. cross sectional study\n",
    "- long. study - part. followed over period of time to examine effect of treatment\n",
    "- cross sect study - data collected from a single snapshot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
